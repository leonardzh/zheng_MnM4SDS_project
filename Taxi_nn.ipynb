{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\geo\\lib\\site-packages\\pysal\\explore\\segregation\\network\\network.py:16: UserWarning: You need pandana and urbanaccess to work with segregation's network module\n",
      "You can install them with  `pip install urbanaccess pandana` or `conda install -c udst pandana urbanaccess`\n",
      "  \"You need pandana and urbanaccess to work with segregation's network module\\n\"\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "# This ensures visualizations are plotted inside the notebook\n",
    "%matplotlib inline\n",
    "import io\n",
    "import os              # This provides several system utilities\n",
    "import pandas as pd    \n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import rtree\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from cenpy import products\n",
    "import cenpy\n",
    "import scipy.stats  as stats # low-level stats & probability\n",
    "import statsmodels.formula.api as smf # high-level stats\n",
    "import requests\n",
    "import contextily as ctx\n",
    "import rasterio as rio\n",
    "import pysal as ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-05.csv\n",
    "taxi_data = pd.read_csv('yellow_tripdata_2019-05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data[\"pickuptime\"] = pd.to_datetime(taxi_data[\"tpep_pickup_datetime\"])\n",
    "taxi_data[\"dropofftime\"] = pd.to_datetime(taxi_data[\"tpep_dropoff_datetime\"])\n",
    "taxi_data[\"hour\"] = taxi_data[\"pickuptime\"].map(lambda x: x.hour)\n",
    "taxi_data[\"weekday\"] = taxi_data[\"pickuptime\"].map(lambda x: x.weekday)\n",
    "taxi_data[\"weekday_name\"] = taxi_data[\"pickuptime\"].map(lambda x: x.day_name)\n",
    "taxi_lite = taxi_data[['PULocationID','DOLocationID','trip_distance','pickuptime','dropofftime','hour','weekday_name']]\n",
    "week_mask = (taxi_lite['pickuptime'] >= '2019-5-6') & (taxi_lite['pickuptime'] < '2019-5-13')\n",
    "#get sample data of one week\n",
    "sampleweek_taxi_lite = taxi_lite.loc[week_mask]\n",
    "#get hourly count\n",
    "hour_si = pd.DataFrame({'count' : sampleweek_taxi_lite.groupby(['PULocationID','DOLocationID','hour'] ).size()}).reset_index()\n",
    "hour_si = hour_si[hour_si['PULocationID']!=hour_si['DOLocationID']]\n",
    "hour_si = hour_si[ (hour_si['PULocationID']<264) & (hour_si['DOLocationID']<264)]\n",
    "# due to missing records of 264 and 265 in shp\n",
    "\n",
    "#get distances\n",
    "taxi_zone = gpd.read_file('taxi_zones/taxi_zones.shp')\n",
    "centroids =  ps.lib.weights.get_points_array_from_shapefile(\"taxi_zones/taxi_zones.shp\")\n",
    "dist_mat = ps.lib.cg.distance_matrix(centroids)\n",
    "dist_mat\n",
    "\n",
    "for i, row in hour_si.iterrows():\n",
    "    hour_si.at[i,'Dij'] = dist_mat[int(row['PULocationID']-1)][int(row['DOLocationID']-1)]\n",
    "#generate final dataframe containing O,D,Dist,flow\n",
    "df = pd.DataFrame()\n",
    "for hr in range(24):\n",
    "    hour1 = hour_si[hour_si['hour'] == hr]\n",
    "    \n",
    "    res_do = pd.DataFrame(hour1.groupby('DOLocationID')['count'].count())\n",
    "    res_do['hour'] = hr * np.ones(res_do.shape)\n",
    "    res_do = res_do.astype({'hour': 'int32'})\n",
    "    res_do = res_do.rename(columns = {'count':'Dj'})\n",
    "    \n",
    "    res_pu = pd.DataFrame(hour1.groupby('PULocationID')['count'].count())\n",
    "    res_pu['hour'] = hr * np.ones(res_pu.shape)\n",
    "    res_pu = res_pu.astype({'hour': 'int32'})\n",
    "    res_pu = res_pu.rename(columns = {'count':'Oi'})\n",
    "    \n",
    "    hour1_O = pd.merge(hour1, res_pu,how = 'left',on=['PULocationID', 'hour'])\n",
    "    hour1_OD = pd.merge(hour1_O, res_do,how = 'left',on=['DOLocationID', 'hour'])\n",
    "    hour1_OD = hour1_OD.rename(columns={'count':'flow'})\n",
    "    #print(hour1_OD.head())\n",
    "    df = df.append(hour1_OD)\n",
    "df\n",
    "# find (PU,DO) that has hour [0,...,23]\n",
    "grouped = df.groupby(['PULocationID','DOLocationID'])\n",
    "\n",
    "df2 = grouped.aggregate(lambda x: np.array(x))\n",
    "df_full = df2[df2['hour'].apply(lambda x: x.size) ==24]\n",
    "df_clear_full = df_full.reset_index()[['Oi','Dj','Dij','flow']]\n",
    "\n",
    "Y = df_clear_full['flow'].to_numpy()\n",
    "Y = np.stack(Y, axis=0)\n",
    "Y[0].reshape(24,-1,1)\n",
    "\n",
    "X_pd = df_clear_full[['Oi','Dj','Dij']]\n",
    "\n",
    "X = np.array([], dtype=np.int64).reshape(0,3)\n",
    "for i, row in X_pd.iterrows():\n",
    "    X_row = np.array( [row['Oi'],row['Dj'],row['Dij']])\n",
    "#     print(X_row.T)\n",
    "#     print(X_row.T.shape)\n",
    "    X = np.vstack([X, X_row.T]) if X.size else X_row.T\n",
    "#     if i > 2:\n",
    "#         break\n",
    "X_re1 = X.reshape(-1,24,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 72)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_d2 = X_re1[:1000,].reshape(-1,24*3)\n",
    "X_d2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_d2[:,0:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_head = X_d2[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y[:1000,].reshape(-1,24,1,1)\n",
    "Y.shape\n",
    "\n",
    "Y_d2 = Y.reshape(-1,24*1)\n",
    "Y_d2.shape\n",
    "\n",
    "Y_d2\n",
    "\n",
    "Y_d2[:,-1].shape\n",
    "Y_tail = Y_d2[:,-1]\n",
    "\n",
    "Y_head = Y_d2[:,0].reshape(-1,1)\n",
    "Y_head.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "normed_Y = normalize(Y_head, axis=0, norm='l1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('simple_X',X_head)\n",
    "np.save('simple_Y',Y_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch to implement LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22ed4cda5f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 1)  # Input dim is 3, output dim is 3\n",
    "inputs = torch.from_numpy(X_d4).float()\n",
    "\n",
    "print(inputs[0])\n",
    "\n",
    "#BUG: double input variables not accepted by LSTM, got error message:\n",
    "# RuntimeError: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "split_id = int(split_frac * len(X_d2))\n",
    "val_X, test_X = X_d2[:split_id], X_d2[split_id:]\n",
    "val_Y, test_Y = Y_tail[:split_id], Y_tail[split_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(test_X).float(), torch.from_numpy(test_Y).float())\n",
    "val_data = TensorDataset(torch.from_numpy(val_X).float(), torch.from_numpy(val_Y).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   60.        ,    68.        ,  2857.6722853 , ...,\n",
       "           63.        ,    71.        ,  2857.6722853 ],\n",
       "       [   33.        ,    59.        ,  9644.64563124, ...,\n",
       "           24.        ,    55.        ,  9644.64563124],\n",
       "       [   33.        ,    38.        ,  4955.55320952, ...,\n",
       "           24.        ,    37.        ,  4955.55320952],\n",
       "       ...,\n",
       "       [   82.        ,    52.        , 15915.83056906, ...,\n",
       "           91.        ,    56.        , 15915.83056906],\n",
       "       [   82.        ,    60.        ,  8932.67314552, ...,\n",
       "           91.        ,    58.        ,  8932.67314552],\n",
       "       [   82.        ,    56.        , 11496.0026935 , ...,\n",
       "           91.        ,    67.        , 11496.0026935 ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSINet(nn.Module):\n",
    "    def __init__(self, input_size,batch_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(TSINet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x,hidden):\n",
    "        x=x.t()\n",
    "        x = x.reshape(x.size(0),self.batch_size, -1)\n",
    "#         print('input shape')\n",
    "#         print(x.shape)\n",
    "\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "#        print('lstm out and hidden0 shape')\n",
    "#         print(lstm_out.shape)\n",
    "#         print(hidden[0].shape)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "output_size = 1\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "\n",
    "model = TSINet(input_size,batch_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "seq_length = 72\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(seq_length)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "#         print(inputs.t().shape)\n",
    "\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(seq_length)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 1),\n",
    "          torch.randn(1, 1, 1))\n",
    "\n",
    "\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs[0]:\n",
    "    print(i)\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "    print(out)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.5174, 1.6815, 0.6686]]), tensor([[-1.3771, -0.4192, -1.4726]]), tensor([[-0.4775,  0.5501, -1.1967]]), tensor([[ 0.0489, -1.8084,  2.0058]]), tensor([[0.9735, 0.5364, 0.7165]]), tensor([[ 1.2615, -0.0659,  2.3018]]), tensor([[-1.4944, -1.0577, -1.2188]]), tensor([[0.7033, 1.0838, 1.4373]]), tensor([[0.4898, 1.7899, 0.9227]]), tensor([[ 1.4088, -0.4567,  0.1100]]), tensor([[-0.2419,  0.2401,  1.4360]]), tensor([[-0.0668,  0.8720,  0.6791]]), tensor([[-0.4543, -0.2149, -1.8579]]), tensor([[-0.4278,  1.8769,  0.8630]]), tensor([[0.0771, 0.9061, 1.2043]]), tensor([[ 1.3060,  0.5141, -1.2784]]), tensor([[-1.3969, -1.2800, -0.2468]]), tensor([[-0.4159,  1.1004,  0.1369]]), tensor([[-1.4131,  0.6246, -2.1237]]), tensor([[-0.0387,  0.2975, -1.7019]]), tensor([[-0.1952, -1.8107, -1.9912]]), tensor([[-0.2434, -0.5782,  0.6942]]), tensor([[-1.0025, -0.0356, -1.3917]]), tensor([[-0.3523,  1.9390,  0.4224]])]\n",
      "tensor([[1.5174, 1.6815, 0.6686]])\n",
      "tensor([[-1.3771, -0.4192, -1.4726]])\n",
      "tensor([[-0.4775,  0.5501, -1.1967]])\n",
      "tensor([[ 0.0489, -1.8084,  2.0058]])\n",
      "tensor([[0.9735, 0.5364, 0.7165]])\n",
      "tensor([[ 1.2615, -0.0659,  2.3018]])\n",
      "tensor([[-1.4944, -1.0577, -1.2188]])\n",
      "tensor([[0.7033, 1.0838, 1.4373]])\n",
      "tensor([[0.4898, 1.7899, 0.9227]])\n",
      "tensor([[ 1.4088, -0.4567,  0.1100]])\n",
      "tensor([[-0.2419,  0.2401,  1.4360]])\n",
      "tensor([[-0.0668,  0.8720,  0.6791]])\n",
      "tensor([[-0.4543, -0.2149, -1.8579]])\n",
      "tensor([[-0.4278,  1.8769,  0.8630]])\n",
      "tensor([[0.0771, 0.9061, 1.2043]])\n",
      "tensor([[ 1.3060,  0.5141, -1.2784]])\n",
      "tensor([[-1.3969, -1.2800, -0.2468]])\n",
      "tensor([[-0.4159,  1.1004,  0.1369]])\n",
      "tensor([[-1.4131,  0.6246, -2.1237]])\n",
      "tensor([[-0.0387,  0.2975, -1.7019]])\n",
      "tensor([[-0.1952, -1.8107, -1.9912]])\n",
      "tensor([[-0.2434, -0.5782,  0.6942]])\n",
      "tensor([[-1.0025, -0.0356, -1.3917]])\n",
      "tensor([[-0.3523,  1.9390,  0.4224]])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 1)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(24)]  # make a sequence of length 5\n",
    "print(inputs)\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 1),\n",
    "          torch.randn(1, 1, 1))\n",
    "for i in inputs:\n",
    "    print(i)\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://www.jessicayung.com/lstms-for-time-series-in-pytorch/\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1)\n",
    "\n",
    "model = LSTM(72, 64, batch_size=4, output_dim=1, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "hist = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_d4).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0611], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "model.hidden = model.init_hidden()\n",
    "y_pred = model(X_train)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Clear stored gradient\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if t % 100 == 0:\n",
    "        print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def sigmoid_derivative(X):\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_grad(z):\n",
    "     return 1 - np.tanh(z) ** 2\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.clip(z, 0, np.inf)\n",
    "\n",
    "def ReLU_grad(z):\n",
    "    return (z > 0).astype(int)\n",
    "\n",
    "def affine(X,slope=1,intercept=0):\n",
    "     return slope * X + intercept\n",
    "    \n",
    "def affine_grad(X,slope=1,intercept=0):\n",
    "    return slope * np.ones_like(X)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, output_dim=1,lr=0.005):\n",
    "        \n",
    "        self.weights1   = np.random.rand(input_dim,4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        \n",
    "        self.lr         = lr\n",
    "    def print_w(self):\n",
    "        print('print_w:')\n",
    "        print('w:')\n",
    "        print(self.weights1)\n",
    "        print(self.weights2)\n",
    "    def feedforward(self,X):\n",
    "        self.layer1 = sigmoid(np.dot(X, self.weights1))\n",
    "#         print('layer1')\n",
    "#         print(self.layer1)\n",
    "#         print(np.dot(self.layer1, self.weights2))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        \n",
    "#         print('forwardout')\n",
    "#         print(self.output)\n",
    "    def backprop(self,X, Y):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(Y - self.output) * sigmoid_derivative(np.dot(self.layer1, self.weights2))))\n",
    "        d_weights1 = np.dot(X.T,  \\\n",
    "                            (np.dot(2*(Y - self.output) * sigmoid_derivative(np.dot(self.layer1, self.weights2)), self.weights2.T)\\\n",
    "                             * sigmoid_derivative(np.dot(X, self.weights1))))\n",
    "#         print('d_w1')\n",
    "#         print(d_weights1)\n",
    "#         print('d_w2')\n",
    "#         print(d_weights2)\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1*self.lr\n",
    "        self.weights2 += d_weights2*self.lr\n",
    "    def test(self,data):\n",
    "        #print(data)\n",
    "        out_layer1 = sigmoid(np.dot(data, self.weights1))\n",
    "        return sigmoid(np.dot(out_layer1, self.weights2))\n",
    "        \n",
    "    def train(self,X,Y,num_train_iterations): \n",
    "        for iteration in range(num_train_iterations): \n",
    "            self.feedforward(X) \n",
    "            #self.print_w()\n",
    "            self.backprop(X,Y)\n",
    "            #self.print_w()\n",
    "            if iteration % 100000 == 0:\n",
    "                mse = np.mean((self.output - Y)**2)\n",
    "                print(\"Epoch \", iteration, \"MSE: \", mse)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('simple_X.npy')\n",
    "Y = np.load('simple_Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('trivial_add_in.npy')\n",
    "y_train = np.load('trivial_add_out.npy')\n",
    "X_test  = np.load('trivial_add_tin.npy')[:10,]\n",
    "y_test  = np.load('trivial_add_tout.npy')[:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'X1': X_train[:, 0], 'X2': X_train[:, 1],'X3': X_train[:,2], 'y':y_train.reshape(-1)})\n",
    "model = smf.ols('y~X1+X2+X3', data=dataset).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.208e+32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 18 Nov 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:32:56</td>     <th>  Log-Likelihood:    </th> <td>  3578.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -7148.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    96</td>      <th>  BIC:               </th> <td>  -7138.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td> 2.776e-17</td> <td> 2.12e-17</td> <td>    1.312</td> <td> 0.193</td> <td>-1.42e-17</td> <td> 6.97e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X1</th>        <td>    0.3333</td> <td> 2.38e-17</td> <td>  1.4e+16</td> <td> 0.000</td> <td>    0.333</td> <td>    0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X2</th>        <td>    0.3333</td> <td> 2.55e-17</td> <td> 1.31e+16</td> <td> 0.000</td> <td>    0.333</td> <td>    0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X3</th>        <td>    0.3333</td> <td> 2.47e-17</td> <td> 1.35e+16</td> <td> 0.000</td> <td>    0.333</td> <td>    0.333</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.185</td> <th>  Durbin-Watson:     </th> <td>   2.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.335</td> <th>  Jarque-Bera (JB):  </th> <td>   1.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.321</td> <th>  Prob(JB):          </th> <td>   0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.114</td> <th>  Cond. No.          </th> <td>    5.82</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                  1.000\n",
       "Method:                 Least Squares   F-statistic:                 2.208e+32\n",
       "Date:                Mon, 18 Nov 2019   Prob (F-statistic):               0.00\n",
       "Time:                        22:32:56   Log-Likelihood:                 3578.2\n",
       "No. Observations:                 100   AIC:                            -7148.\n",
       "Df Residuals:                      96   BIC:                            -7138.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept   2.776e-17   2.12e-17      1.312      0.193   -1.42e-17    6.97e-17\n",
       "X1             0.3333   2.38e-17    1.4e+16      0.000       0.333       0.333\n",
       "X2             0.3333   2.55e-17   1.31e+16      0.000       0.333       0.333\n",
       "X3             0.3333   2.47e-17   1.35e+16      0.000       0.333       0.333\n",
       "==============================================================================\n",
       "Omnibus:                        2.185   Durbin-Watson:                   2.203\n",
       "Prob(Omnibus):                  0.335   Jarque-Bera (JB):                1.771\n",
       "Skew:                           0.321   Prob(JB):                        0.412\n",
       "Kurtosis:                       3.114   Cond. No.                         5.82\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For trivial dataset\n",
    "# X_train = np.load('trivial_in.npy')[:100,]\n",
    "# y_train = np.load('trivial_out.npy')[:100,]\n",
    "# X_test = np.array([[1, 0, 0]])\n",
    "# y_test = np.array([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  0.09126839943463383\n",
      "Epoch  100000 MSE:  9.678953528512493e-30\n",
      "Epoch  200000 MSE:  4.314083075427408e-31\n",
      "Epoch  300000 MSE:  4.314083075427408e-31\n",
      "Epoch  400000 MSE:  4.314083075427408e-31\n",
      "print_w:\n",
      "w:\n",
      "[[-0.3840239   0.86346254 -0.09180607  2.18339648]\n",
      " [-1.12348837  0.40654829  0.72032576  0.30371878]\n",
      " [-1.36394178  0.90924602  0.29380448 -0.70813657]]\n",
      "[[-6.69712411]\n",
      " [ 0.49606867]\n",
      " [-0.57414312]\n",
      " [ 1.8580863 ]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.71195239]\n",
      " [0.33748515]\n",
      " [0.32905681]\n",
      " [0.46220717]\n",
      " [0.3976536 ]\n",
      " [0.43155132]\n",
      " [0.46416862]\n",
      " [0.65344375]\n",
      " [0.52042258]\n",
      " [0.35861529]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.001431107238843359\n",
      "Epoch  0 MSE:  0.0010553879929466445\n",
      "Epoch  100000 MSE:  2.894118807701927e-13\n",
      "Epoch  200000 MSE:  1.6108386110134998e-21\n",
      "Epoch  300000 MSE:  3.7197410945895073e-29\n",
      "Epoch  400000 MSE:  1.2418396281408897e-30\n",
      "print_w:\n",
      "w:\n",
      "[[-0.03065492  0.89734398 -0.10964487  1.53163366]\n",
      " [-1.18066836  0.51338466  0.60386236 -0.03993968]\n",
      " [-0.74018282  0.87621357  0.3769639  -1.27314128]]\n",
      "[[-6.39323165]\n",
      " [ 0.89823824]\n",
      " [-0.06815605]\n",
      " [ 2.41244217]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.71740615]\n",
      " [0.43213816]\n",
      " [0.42844911]\n",
      " [0.4494459 ]\n",
      " [0.49834449]\n",
      " [0.40180868]\n",
      " [0.58755763]\n",
      " [0.67263859]\n",
      " [0.54133536]\n",
      " [0.43801991]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.0050637129947109834\n",
      "Epoch  0 MSE:  0.00487653154947046\n",
      "Epoch  100000 MSE:  4.899565778521128e-31\n",
      "Epoch  200000 MSE:  4.899565778521128e-31\n",
      "Epoch  300000 MSE:  4.899565778521128e-31\n",
      "Epoch  400000 MSE:  4.899565778521128e-31\n",
      "print_w:\n",
      "w:\n",
      "[[-0.28232687  0.88202385 -0.17001611  1.87345127]\n",
      " [-1.08226017  0.46190527  0.58627774 -0.14375968]\n",
      " [-1.30002374  0.8934331   0.34541507 -1.08902587]]\n",
      "[[-6.97545903]\n",
      " [ 0.65038724]\n",
      " [-0.4554242 ]\n",
      " [ 2.16643257]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.69310882]\n",
      " [0.35595791]\n",
      " [0.35080904]\n",
      " [0.47638557]\n",
      " [0.41623294]\n",
      " [0.42270791]\n",
      " [0.45412951]\n",
      " [0.65736599]\n",
      " [0.49665272]\n",
      " [0.353871  ]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.0017861164577188434\n",
      "Epoch  0 MSE:  0.000265490678138614\n",
      "Epoch  100000 MSE:  8.782240546405795e-32\n",
      "Epoch  200000 MSE:  8.782240546405795e-32\n",
      "Epoch  300000 MSE:  8.782240546405795e-32\n",
      "Epoch  400000 MSE:  8.782240546405795e-32\n",
      "print_w:\n",
      "w:\n",
      "[[-0.16854959  0.84844737 -0.14035951  1.54877538]\n",
      " [-0.98397048  0.47515724  0.5863801  -0.2383567 ]\n",
      " [-1.19462734  0.92076261  0.34603443 -1.17746075]]\n",
      "[[-6.75400786]\n",
      " [ 0.81137419]\n",
      " [-0.23548735]\n",
      " [ 2.18064404]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.6808885 ]\n",
      " [0.35910125]\n",
      " [0.35299822]\n",
      " [0.46120473]\n",
      " [0.41381579]\n",
      " [0.42489702]\n",
      " [0.46863921]\n",
      " [0.63726814]\n",
      " [0.5108308 ]\n",
      " [0.37602355]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.0021546666127533814\n",
      "Epoch  0 MSE:  0.0027210837777134\n",
      "Epoch  100000 MSE:  7.865497892877471e-31\n",
      "Epoch  200000 MSE:  7.865497892877471e-31\n",
      "Epoch  300000 MSE:  7.865497892877471e-31\n",
      "Epoch  400000 MSE:  7.865497892877471e-31\n",
      "print_w:\n",
      "w:\n",
      "[[-0.45800044  0.88553124 -0.22135928  1.99546858]\n",
      " [-0.93414166  0.53812846  0.49998707  0.54061619]\n",
      " [-1.15670388  0.9654379   0.24375049 -0.55578568]]\n",
      "[[-7.79039242]\n",
      " [ 0.90610656]\n",
      " [-0.44254363]\n",
      " [ 1.9696605 ]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.78153122]\n",
      " [0.35550623]\n",
      " [0.35631287]\n",
      " [0.49995636]\n",
      " [0.42140789]\n",
      " [0.4480948 ]\n",
      " [0.49256593]\n",
      " [0.70676437]\n",
      " [0.56990751]\n",
      " [0.36347342]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.0006024662932903451\n",
      "Epoch  0 MSE:  0.0015453885086149195\n",
      "Epoch  100000 MSE:  5.546678239835239e-32\n",
      "Epoch  200000 MSE:  5.546678239835239e-32\n",
      "Epoch  300000 MSE:  5.546678239835239e-32\n",
      "Epoch  400000 MSE:  5.546678239835239e-32\n",
      "print_w:\n",
      "w:\n",
      "[[ 0.37589253  0.79694209 -0.45945516  2.39170882]\n",
      " [-1.44908119  0.57873205  0.19264525  1.00804541]\n",
      " [-0.99195356  0.85656005  0.02149057 -1.13525274]]\n",
      "[[-8.91886742]\n",
      " [ 1.26301007]\n",
      " [-0.62849557]\n",
      " [ 2.75009456]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.87253796]\n",
      " [0.22678941]\n",
      " [0.17572063]\n",
      " [0.23146481]\n",
      " [0.33385216]\n",
      " [0.3513407 ]\n",
      " [0.73595273]\n",
      " [0.73960471]\n",
      " [0.69367245]\n",
      " [0.48915757]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.02500812819432549\n",
      "Epoch  0 MSE:  0.02094295654759559\n",
      "Epoch  100000 MSE:  5.623715437610729e-32\n",
      "Epoch  200000 MSE:  5.623715437610729e-32\n",
      "Epoch  300000 MSE:  5.623715437610729e-32\n",
      "Epoch  400000 MSE:  5.623715437610729e-32\n",
      "print_w:\n",
      "w:\n",
      "[[-0.14557125  0.82628197 -0.57436376  2.34523537]\n",
      " [-0.77495101  0.46861999  0.17949048  0.67881948]\n",
      " [-1.38760865  0.88200597 -0.05422658 -1.13086361]]\n",
      "[[-9.14833773]\n",
      " [ 1.19386119]\n",
      " [-0.89891845]\n",
      " [ 2.62483024]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.82764586]\n",
      " [0.28337721]\n",
      " [0.26333527]\n",
      " [0.47152469]\n",
      " [0.35277235]\n",
      " [0.44431856]\n",
      " [0.47375373]\n",
      " [0.73487169]\n",
      " [0.5743332 ]\n",
      " [0.33549607]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.002662730209219217\n",
      "Epoch  0 MSE:  0.0024793440429911186\n",
      "Epoch  100000 MSE:  1.2422248141297671e-31\n",
      "Epoch  200000 MSE:  1.2422248141297671e-31\n",
      "Epoch  300000 MSE:  1.2422248141297671e-31\n",
      "Epoch  400000 MSE:  1.2422248141297671e-31\n",
      "print_w:\n",
      "w:\n",
      "[[-0.29582663  0.84311818 -0.57780604  2.34574579]\n",
      " [-0.36459037  0.4137573   0.22930091  0.55341881]\n",
      " [-1.10484484  0.84605033 -0.01970152 -1.18942575]]\n",
      "[[-8.89114456]\n",
      " [ 1.32503596]\n",
      " [-0.7311904 ]\n",
      " [ 2.81400989]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.77554732]\n",
      " [0.37764418]\n",
      " [0.39109062]\n",
      " [0.58202401]\n",
      " [0.42010179]\n",
      " [0.44210409]\n",
      " [0.34940768]\n",
      " [0.71004014]\n",
      " [0.4613663 ]\n",
      " [0.26935946]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.004182584957799208\n",
      "Epoch  0 MSE:  0.005622100538750378\n",
      "Epoch  100000 MSE:  1.1093356479670479e-31\n",
      "Epoch  200000 MSE:  1.1093356479670479e-31\n",
      "Epoch  300000 MSE:  1.1093356479670479e-31\n",
      "Epoch  400000 MSE:  1.1093356479670479e-31\n",
      "print_w:\n",
      "w:\n",
      "[[ 3.60348548e-02  7.97580674e-01 -5.50952793e-01  2.30752316e+00]\n",
      " [-6.44224151e-01  4.44815490e-01  2.31413372e-01  5.46389895e-01]\n",
      " [-1.04408095e+00  8.20876861e-01  7.59813939e-04 -1.26444711e+00]]\n",
      "[[-8.71636916]\n",
      " [ 1.4106603 ]\n",
      " [-0.56827294]\n",
      " [ 2.91632504]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.80239428]\n",
      " [0.33525924]\n",
      " [0.31827382]\n",
      " [0.4658574 ]\n",
      " [0.39684418]\n",
      " [0.42061307]\n",
      " [0.49063333]\n",
      " [0.70901726]\n",
      " [0.53788653]\n",
      " [0.34773852]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.0006112736241482301\n",
      "Epoch  0 MSE:  0.001693883818378813\n",
      "Epoch  100000 MSE:  2.868648266225271e-07\n",
      "Epoch  200000 MSE:  6.277329721018771e-10\n",
      "Epoch  300000 MSE:  1.3766960954270135e-12\n",
      "Epoch  400000 MSE:  3.0198271925804446e-15\n",
      "print_w:\n",
      "w:\n",
      "[[ 0.29119732  0.63689399 -0.7425098   1.91575912]\n",
      " [-0.19619817  0.27393457 -0.39033818  2.16977307]\n",
      " [-2.03179257  0.83388028 -0.65056115 -2.66721669]]\n",
      "[[-12.98619806]\n",
      " [  2.0744458 ]\n",
      " [ -4.70694279]\n",
      " [  4.22225735]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.8642971 ]\n",
      " [0.04248905]\n",
      " [0.02989002]\n",
      " [0.17374244]\n",
      " [0.06401174]\n",
      " [0.20142591]\n",
      " [0.25206855]\n",
      " [0.62686605]\n",
      " [0.48111484]\n",
      " [0.14984689]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.05662768793435132\n",
      "Epoch  0 MSE:  0.06435817437974445\n",
      "Epoch  100000 MSE:  2.2340787354891936e-32\n",
      "Epoch  200000 MSE:  2.2340787354891936e-32\n",
      "Epoch  300000 MSE:  2.2340787354891936e-32\n",
      "Epoch  400000 MSE:  2.2340787354891936e-32\n",
      "print_w:\n",
      "w:\n",
      "[[-0.16658554  0.37925548 -0.17404428  2.00228095]\n",
      " [-0.88276894 -0.05154613  0.38149499  1.05717647]\n",
      " [-3.07243221  0.60407388 -0.04064475 -3.41486197]]\n",
      "[[-12.10276428]\n",
      " [  2.45433547]\n",
      " [ -3.90579485]\n",
      " [  4.8063374 ]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.71796834]\n",
      " [0.29691462]\n",
      " [0.2437337 ]\n",
      " [0.64813138]\n",
      " [0.40797339]\n",
      " [0.407188  ]\n",
      " [0.51813256]\n",
      " [0.8125968 ]\n",
      " [0.42948004]\n",
      " [0.30948547]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.009819597051938352\n",
      "Epoch  0 MSE:  0.02605948236374287\n",
      "Epoch  100000 MSE:  9.244463733058732e-32\n",
      "Epoch  200000 MSE:  9.244463733058732e-32\n",
      "Epoch  300000 MSE:  9.244463733058732e-32\n",
      "Epoch  400000 MSE:  9.244463733058732e-32\n",
      "print_w:\n",
      "w:\n",
      "[[-0.76002976 -0.28692609  0.77158802  1.64695354]\n",
      " [ 0.25093037 -0.14417511  0.25010478  1.85123035]\n",
      " [-2.98881365  0.5244634  -0.03583228 -2.33042491]]\n",
      "[[-11.75102466]\n",
      " [  2.65113702]\n",
      " [ -3.75702049]\n",
      " [  4.47460055]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.79334367]\n",
      " [0.22208812]\n",
      " [0.2048257 ]\n",
      " [0.57094046]\n",
      " [0.24966995]\n",
      " [0.45616645]\n",
      " [0.23568175]\n",
      " [0.72443921]\n",
      " [0.56178361]\n",
      " [0.28107335]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.01412174097289012\n",
      "Epoch  0 MSE:  0.004512598894230713\n",
      "Epoch  100000 MSE:  3.851859888774472e-33\n",
      "Epoch  200000 MSE:  3.851859888774472e-33\n",
      "Epoch  300000 MSE:  3.851859888774472e-33\n",
      "Epoch  400000 MSE:  3.851859888774472e-33\n",
      "print_w:\n",
      "w:\n",
      "[[-1.11694206 -0.27970367  0.74140633  1.39752217]\n",
      " [ 0.13193359 -0.27706763  0.41228707  1.51691924]\n",
      " [-3.01152588  0.57708927 -0.16248239 -2.43403956]]\n",
      "[[-11.70821608]\n",
      " [  2.76343292]\n",
      " [ -3.6771498 ]\n",
      " [  4.42257264]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.72806063]\n",
      " [0.33620768]\n",
      " [0.34866863]\n",
      " [0.63320336]\n",
      " [0.36779699]\n",
      " [0.4848675 ]\n",
      " [0.24488046]\n",
      " [0.71974964]\n",
      " [0.51909875]\n",
      " [0.27953702]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.009395981245039129\n",
      "Epoch  0 MSE:  0.007292221231167685\n",
      "Epoch  100000 MSE:  1.4752383788321145e-26\n",
      "Epoch  200000 MSE:  1.4752383788321145e-26\n",
      "Epoch  300000 MSE:  1.4752383788321145e-26\n",
      "Epoch  400000 MSE:  1.4752383788321145e-26\n",
      "print_w:\n",
      "w:\n",
      "[[-0.33655689  0.17367948  0.21285274  1.97541412]\n",
      " [ 0.8952268   0.0781287  -0.00617343  2.28654709]\n",
      " [-3.13374579  0.48100054  0.1522267  -2.72551218]]\n",
      "[[-11.76561445]\n",
      " [  2.54963007]\n",
      " [ -3.67899235]\n",
      " [  4.95993282]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.87953674]\n",
      " [0.23457186]\n",
      " [0.21211214]\n",
      " [0.71999197]\n",
      " [0.23049014]\n",
      " [0.50758818]\n",
      " [0.13364188]\n",
      " [0.80272027]\n",
      " [0.57447708]\n",
      " [0.2517414 ]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.027919558070875827\n",
      "Epoch  0 MSE:  0.006906209950286211\n",
      "Epoch  100000 MSE:  4.622231866529366e-33\n",
      "Epoch  200000 MSE:  4.622231866529366e-33\n",
      "Epoch  300000 MSE:  4.622231866529366e-33\n",
      "Epoch  400000 MSE:  4.622231866529366e-33\n",
      "print_w:\n",
      "w:\n",
      "[[-0.06832463  0.04297666  0.40603215  1.69053279]\n",
      " [ 0.99883146  0.06538852  0.02229768  2.26878598]\n",
      " [-3.27687246  0.46979529  0.17330814 -2.86531604]]\n",
      "[[-11.67843423]\n",
      " [  2.63314934]\n",
      " [ -3.60225338]\n",
      " [  4.97922967]]\n",
      "Testing MSE on new examples ->\n",
      "[[0.79669596]\n",
      " [0.11535918]\n",
      " [0.08923175]\n",
      " [0.51640568]\n",
      " [0.11185691]\n",
      " [0.42275835]\n",
      " [0.10085771]\n",
      " [0.65413392]\n",
      " [0.51813977]\n",
      " [0.22590021]]\n",
      "[[0.8147038 ]\n",
      " [0.35870648]\n",
      " [0.37194116]\n",
      " [0.47298015]\n",
      " [0.40965842]\n",
      " [0.41951059]\n",
      " [0.45831198]\n",
      " [0.67439782]\n",
      " [0.54407605]\n",
      " [0.35353644]]\n",
      "0.03751967497909442\n",
      "Epoch  0 MSE:  0.018222066800251777\n",
      "Epoch  100000 MSE:  6.933347799794049e-33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-6287c978f460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mneural_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mneural_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-3aad62e0b1a7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, Y, num_train_iterations)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m#self.print_w()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;31m#self.print_w()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-3aad62e0b1a7>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     50\u001b[0m         d_weights1 = np.dot(X.T,  \\\n\u001b[0;32m     51\u001b[0m                             (np.dot(2*(Y - self.output) * sigmoid_derivative(np.dot(self.layer1, self.weights2)), self.weights2.T)\\\n\u001b[1;32m---> 52\u001b[1;33m                              * sigmoid_derivative(np.dot(X, self.weights1))))\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;31m#         print('d_w1')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m#         print(d_weights1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "# X_test_slice = X_test[:10,]\n",
    "# y_test_slice = y_test[:10,]\n",
    "neural_network = NeuralNetwork(3,1,lr=1)\n",
    "\n",
    "for index in range(0,X_train.shape[0],batch_size):\n",
    "    batch_X=X_train[index:min(index+batch_size,X_train.shape[0]),:]\n",
    "    batch_Y=y_train[index:min(index+batch_size,y_train.shape[0])]\n",
    "\n",
    "\n",
    "    neural_network.train(batch_X,batch_Y,500000) \n",
    "    neural_network.print_w()\n",
    "\n",
    "    # Test the neural network with a new situation. \n",
    "    print (\"Testing MSE on new examples ->\") \n",
    "    \n",
    "    y_pred = neural_network.test(X_test)\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    mse = np.mean((y_test - y_pred)**2)\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        if cuda_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            model.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 400 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1) * len(data), len(data_loader.dataset),\n",
    "                100. * (batch_idx+1) / len(data_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, criterion, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        if cuda_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            model.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader) # loss function already averages over batch size\n",
    "    acc = correct / len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset), 100. * acc))\n",
    "    return (acc, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1200316 , -0.29743934, -0.04680083, -1.09951908,  0.93325838,\n",
       "       -0.96146334, -0.48316631,  1.31597987,  0.97514011,  0.72879973,\n",
       "        0.03357932, -0.24503645, -0.39372671,  0.65938907,  0.34883533,\n",
       "       -0.64919665, -0.72613393, -0.01364653,  0.63252934,  0.0204372 ,\n",
       "       -0.12778696, -0.31003516, -1.2401461 ,  1.15040962,  0.57146278,\n",
       "        1.36472933, -0.9473326 , -0.38521173, -0.20773179,  0.95023917,\n",
       "        1.02297765, -1.06688455, -0.53688861,  1.08205386, -0.83792327,\n",
       "       -0.15144443, -2.37210933,  0.69739925,  0.02223237, -0.56304962,\n",
       "        1.54098464, -0.88481618,  0.93107118,  0.33258876,  1.13566404,\n",
       "        0.78928099, -0.99949509, -0.74632323,  0.76567017, -1.40157169,\n",
       "       -1.8881677 , -0.49125796,  0.9378908 ,  1.1937171 , -2.1338117 ,\n",
       "       -1.01792292, -0.18666733,  0.29230986, -1.09024831, -0.27043559,\n",
       "       -0.58365002,  0.96089592, -1.09975038,  0.84939133,  0.5452521 ,\n",
       "        0.91265126, -0.09294232,  0.5517376 ,  1.59760682,  1.23567207,\n",
       "        0.22806259,  0.59579698, -1.07975127,  1.57138865, -1.04856117,\n",
       "       -0.02979509, -0.35728797,  0.25333578, -1.22390094,  0.6175919 ,\n",
       "       -0.63909826,  1.82671616, -1.26856061,  0.83345454, -1.12639941,\n",
       "       -0.15539631, -1.32181994, -0.39708334, -0.38008087,  0.48926851,\n",
       "        1.09359126,  0.08163164, -1.26608191, -0.25329941, -2.04338528,\n",
       "       -0.78278691, -0.44439789, -0.07059682,  0.56610753, -0.22839901])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "\n",
    "x_train, y_train, W_target = make_regression(n_samples=100, n_features=1, noise=1, coef = True)\n",
    "x_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = {'X':x_train[:,0].ravel(), 'Y':y_train.ravel()})\n",
    "\n",
    "sns.lmplot(x='X', y='Y', data=df, fit_reg=True)\n",
    "plt.show()\n",
    "\n",
    "x_torch = torch.FloatTensor(x_train)\n",
    "y_torch = torch.FloatTensor(y_train)\n",
    "y_torch = y_torch.view(y_torch.size()[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size,hidden_size=4):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)  \n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleNN(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegression(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal parameters:\n",
      "fc1.weight:  <class 'torch.nn.parameter.Parameter'> torch.Size([4, 1])\n",
      "---------------------------------------------------------------------------------------\n",
      "fc1.bias:  <class 'torch.nn.parameter.Parameter'> torch.Size([4])\n",
      "---------------------------------------------------------------------------------------\n",
      "fc2.weight:  <class 'torch.nn.parameter.Parameter'> torch.Size([1, 4])\n",
      "---------------------------------------------------------------------------------------\n",
      "fc2.bias:  <class 'torch.nn.parameter.Parameter'> torch.Size([1])\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('internal parameters:')\n",
    "for name,param in model.named_parameters():\n",
    "    print(name + \": \", type(param), param.size())\n",
    "    print('---------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  \n",
    "\n",
    "\n",
    "for epoch in range(500):\n",
    "    data, target = Variable(x_torch), Variable(y_torch)\n",
    "    output = model(data)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "predicted = model(Variable(x_torch)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(x_train, y_train, 'o', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([4, 1])\n",
      "Parameter containing:\n",
      "tensor([[ 4.7249],\n",
      "        [ 2.5333],\n",
      "        [ 2.0610],\n",
      "        [-5.2015]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "t = model.fc1.weight\n",
    "print (type(t))\n",
    "print (t.size())\n",
    "print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
