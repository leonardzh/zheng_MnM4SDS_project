{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jessicayung/blog-code-snippets/blob/master/lstm-pytorch/lstm-baseline.py\n",
    "#https://github.com/jessicayung/blog-code-snippets/blob/master/lstm-pytorch/generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to generate autoregressive data.\n",
    "Blog post: http://www.jessicayung.com/generating-autoregressive-data-for-experiments=\n",
    "Author: Jessiac Yung\n",
    "Sept 2018\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TimeSeriesData:\n",
    "    def __init__(self, num_datapoints, test_size=0.2, max_t=20, num_prev=1,\n",
    "                 noise_var=1):\n",
    "        \"\"\"\n",
    "        Template class for generating time series data.\n",
    "        :param test_size: in (0,1), data to be used in test set as a fraction of all data generated.\n",
    "        \"\"\"\n",
    "        self.num_datapoints = num_datapoints\n",
    "        self.test_size = test_size\n",
    "        self.num_prev = num_prev\n",
    "        self.max_t = max_t\n",
    "        self.data = None\n",
    "        self.noise_var = noise_var\n",
    "        self.y = np.zeros(num_datapoints + num_prev*4) # TODO: check this\n",
    "        self.bayes_preds = np.copy(self.y)\n",
    "\n",
    "        # Generate data and reshape data\n",
    "        self.create_data()\n",
    "\n",
    "        # Split into training and test sets\n",
    "        self.train_test_split()\n",
    "\n",
    "    def create_data(self):\n",
    "        self.generate_data()\n",
    "        self.reshape_data()\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"Generates data in self.y, may take as implicit input timesteps self.t.\n",
    "        May also generate Bayes predictions.\"\"\"\n",
    "        raise NotImplementedError(\"Generate data method not implemented.\")\n",
    "\n",
    "    def reshape_data(self):\n",
    "        self.x = np.reshape([self.y[i:i + self.num_prev] for i in range(\n",
    "            self.num_datapoints)], (-1, self.num_prev))\n",
    "        self.y = np.copy(self.y[self.num_prev:])\n",
    "        self.bayes_preds = np.copy(self.bayes_preds[self.num_prev:])\n",
    "\n",
    "    def train_test_split(self):\n",
    "        test_size = int(len(self.y) * self.test_size)\n",
    "        self.data = [self.X_train, self.X_test, self.y_train,\n",
    "                     self.y_test] = \\\n",
    "                    self.x[:-test_size], self.x[-test_size:], \\\n",
    "                    self.y[:-test_size], self.y[-test_size:]\n",
    "        self.bayes_preds = [self.bayes_train_preds, self.bayes_test_preds] = self.bayes_preds[:-test_size], self.bayes_preds[-test_size:]\n",
    "\n",
    "    def return_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def return_train_test(self):\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "class ARData(TimeSeriesData):\n",
    "    \"\"\"Class to generate autoregressive data.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, coeffs=None, **kwargs):\n",
    "        self.given_coeffs = coeffs\n",
    "        super(ARData, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if coeffs is not None:\n",
    "            self.num_prev = len(coeffs) - 1\n",
    "\n",
    "    def generate_data(self):\n",
    "        self.generate_coefficients()\n",
    "        self.generate_initial_points()\n",
    "\n",
    "        # + 3*self.num_prev because we want to cut first (3*self.num_prev) datapoints later\n",
    "        # so dist is more stationary (else initial num_prev datapoints will stand out as diff dist)\n",
    "        for i in range(self.num_datapoints+3*self.num_prev):\n",
    "            # Generate y value if there was no noise\n",
    "            # (equivalent to Bayes predictions: predictions from oracle that knows true parameters (coefficients))\n",
    "            self.bayes_preds[i + self.num_prev] = np.dot(self.y[i:self.num_prev+i][::-1], self.coeffs)\n",
    "            # Add noise\n",
    "            self.y[i + self.num_prev] = self.bayes_preds[i + self.num_prev] + self.noise()\n",
    "\n",
    "        # Cut first 20 points so dist is roughly stationary\n",
    "        self.bayes_preds = self.bayes_preds[3*self.num_prev:]\n",
    "        self.y = self.y[3*self.num_prev:]\n",
    "\n",
    "    def generate_coefficients(self):\n",
    "        if self.given_coeffs is not None:\n",
    "            self.coeffs = self.given_coeffs\n",
    "        else:\n",
    "            filter_stable = False\n",
    "            # Keep generating coefficients until we come across a set of coefficients\n",
    "            # that correspond to stable poles\n",
    "            while not filter_stable:\n",
    "                true_theta = np.random.random(self.num_prev) - 0.5\n",
    "                coefficients = np.append(1, -true_theta)\n",
    "                # check if magnitude of all poles is less than one\n",
    "                if np.max(np.abs(np.roots(coefficients))) < 1:\n",
    "                    filter_stable = True\n",
    "            self.coeffs = true_theta\n",
    "\n",
    "    def generate_initial_points(self):\n",
    "        # Initial datapoints distributed as N(0,1)\n",
    "        self.y[:self.num_prev] = np.random.randn(self.num_prev)\n",
    "\n",
    "    def noise(self):\n",
    "        # Noise distributed as N(0, self.noise_var)\n",
    "        return self.noise_var * np.random.randn()\n",
    "\n",
    "# A set of coefficients that are stable (to produce replicable plots, experiments)\n",
    "fixed_ar_coefficients = {2: [ 0.46152873, -0.29890739],\n",
    "    5: [ 0.02519834, -0.24396899,  0.2785921,   0.14682383,  0.39390468],\n",
    "                        10: [-0.10958935, -0.34564819,  0.3682048,   0.3134046,  -0.21553732,  0.34613629,\n",
    "  0.41916508,  0.0165352,   0.14163503, -0.38844378],\n",
    "                         20: [ 0.1937815,   0.01201026,  0.00464018, -0.21887467, -0.20113385, -0.02322278,\n",
    "  0.34285319, -0.21069086,  0.06604683, -0.22377364,  0.11714593, -0.07122126,\n",
    " -0.16346554,  0.03174824,  0.308584,    0.06881604,  0.24840789, -0.32735569,\n",
    "  0.21939492, 0.3996207 ]}\n",
    "\n",
    "\"\"\"\n",
    "Example of using fixed coefficients (consistency across tests of different models)\n",
    "data = ARData(100, coeffs=fixed_ar_coefficients[5], num_prev=5)\n",
    "plt.plot(data.y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from generate_data import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#####################\n",
    "# Set parameters\n",
    "#####################\n",
    "\n",
    "# Data params\n",
    "noise_var = 0\n",
    "num_datapoints = 100\n",
    "test_size = 0.2\n",
    "num_train = int((1-test_size) * num_datapoints)\n",
    "\n",
    "# Network params\n",
    "input_size = 20\n",
    "# If `per_element` is True, then LSTM reads in one timestep at a time.\n",
    "per_element = True\n",
    "if per_element:\n",
    "    lstm_input_size = 1\n",
    "else:\n",
    "    lstm_input_size = input_size\n",
    "# size of hidden layers\n",
    "h1 = 32\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 500\n",
    "dtype = torch.float\n",
    "\n",
    "#####################\n",
    "# Generate data\n",
    "#####################\n",
    "data = ARData(num_datapoints, num_prev=input_size, test_size=test_size, noise_var=noise_var, coeffs=fixed_ar_coefficients[input_size])\n",
    "\n",
    "# make training and test sets in torch\n",
    "X_train = torch.from_numpy(data.X_train).type(torch.Tensor)\n",
    "X_test = torch.from_numpy(data.X_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(data.y_train).type(torch.Tensor).view(-1)\n",
    "y_test = torch.from_numpy(data.y_test).type(torch.Tensor).view(-1)\n",
    "\n",
    "X_train = X_train.view([input_size, -1, 1])\n",
    "X_test = X_test.view([input_size, -1, 1])\n",
    "\n",
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1)\n",
    "\n",
    "model = LSTM(lstm_input_size, h1, batch_size=num_train, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if t % 100 == 0:\n",
    "        print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()\n",
    "\n",
    "#####################\n",
    "# Plot preds and performance\n",
    "#####################\n",
    "\n",
    "plt.plot(y_pred.detach().numpy(), label=\"Preds\")\n",
    "plt.plot(y_train.detach().numpy(), label=\"Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
