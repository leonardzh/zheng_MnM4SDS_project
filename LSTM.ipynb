{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jessicayung/blog-code-snippets/blob/master/lstm-pytorch/lstm-baseline.py\n",
    "#https://github.com/jessicayung/blog-code-snippets/blob/master/lstm-pytorch/generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to generate autoregressive data.\n",
    "Blog post: http://www.jessicayung.com/generating-autoregressive-data-for-experiments=\n",
    "Author: Jessiac Yung\n",
    "Sept 2018\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TimeSeriesData:\n",
    "    def __init__(self, num_datapoints, test_size=0.2, max_t=20, num_prev=1,\n",
    "                 noise_var=1):\n",
    "        \"\"\"\n",
    "        Template class for generating time series data.\n",
    "        :param test_size: in (0,1), data to be used in test set as a fraction of all data generated.\n",
    "        \"\"\"\n",
    "        self.num_datapoints = num_datapoints\n",
    "        self.test_size = test_size\n",
    "        self.num_prev = num_prev\n",
    "        self.max_t = max_t\n",
    "        self.data = None\n",
    "        self.noise_var = noise_var\n",
    "        self.y = np.zeros(num_datapoints + num_prev*4) # TODO: check this\n",
    "        self.bayes_preds = np.copy(self.y)\n",
    "\n",
    "        # Generate data and reshape data\n",
    "        self.create_data()\n",
    "\n",
    "        # Split into training and test sets\n",
    "        self.train_test_split()\n",
    "\n",
    "    def create_data(self):\n",
    "        self.generate_data()\n",
    "        self.reshape_data()\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"Generates data in self.y, may take as implicit input timesteps self.t.\n",
    "        May also generate Bayes predictions.\"\"\"\n",
    "        raise NotImplementedError(\"Generate data method not implemented.\")\n",
    "\n",
    "    def reshape_data(self):\n",
    "        self.x = np.reshape([self.y[i:i + self.num_prev] for i in range(\n",
    "            self.num_datapoints)], (-1, self.num_prev))\n",
    "        self.y = np.copy(self.y[self.num_prev:])\n",
    "        self.bayes_preds = np.copy(self.bayes_preds[self.num_prev:])\n",
    "\n",
    "    def train_test_split(self):\n",
    "        test_size = int(len(self.y) * self.test_size)\n",
    "        self.data = [self.X_train, self.X_test, self.y_train,\n",
    "                     self.y_test] = \\\n",
    "                    self.x[:-test_size], self.x[-test_size:], \\\n",
    "                    self.y[:-test_size], self.y[-test_size:]\n",
    "        self.bayes_preds = [self.bayes_train_preds, self.bayes_test_preds] = self.bayes_preds[:-test_size], self.bayes_preds[-test_size:]\n",
    "\n",
    "    def return_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def return_train_test(self):\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "class ARData(TimeSeriesData):\n",
    "    \"\"\"Class to generate autoregressive data.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, coeffs=None, **kwargs):\n",
    "        self.given_coeffs = coeffs\n",
    "        super(ARData, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if coeffs is not None:\n",
    "            self.num_prev = len(coeffs) - 1\n",
    "\n",
    "    def generate_data(self):\n",
    "        self.generate_coefficients()\n",
    "        self.generate_initial_points()\n",
    "\n",
    "        # + 3*self.num_prev because we want to cut first (3*self.num_prev) datapoints later\n",
    "        # so dist is more stationary (else initial num_prev datapoints will stand out as diff dist)\n",
    "        for i in range(self.num_datapoints+3*self.num_prev):\n",
    "            # Generate y value if there was no noise\n",
    "            # (equivalent to Bayes predictions: predictions from oracle that knows true parameters (coefficients))\n",
    "            self.bayes_preds[i + self.num_prev] = np.dot(self.y[i:self.num_prev+i][::-1], self.coeffs)\n",
    "            # Add noise\n",
    "            self.y[i + self.num_prev] = self.bayes_preds[i + self.num_prev] + self.noise()\n",
    "\n",
    "        # Cut first 20 points so dist is roughly stationary\n",
    "        self.bayes_preds = self.bayes_preds[3*self.num_prev:]\n",
    "        self.y = self.y[3*self.num_prev:]\n",
    "\n",
    "    def generate_coefficients(self):\n",
    "        if self.given_coeffs is not None:\n",
    "            self.coeffs = self.given_coeffs\n",
    "        else:\n",
    "            filter_stable = False\n",
    "            # Keep generating coefficients until we come across a set of coefficients\n",
    "            # that correspond to stable poles\n",
    "            while not filter_stable:\n",
    "                true_theta = np.random.random(self.num_prev) - 0.5\n",
    "                coefficients = np.append(1, -true_theta)\n",
    "                # check if magnitude of all poles is less than one\n",
    "                if np.max(np.abs(np.roots(coefficients))) < 1:\n",
    "                    filter_stable = True\n",
    "            self.coeffs = true_theta\n",
    "\n",
    "    def generate_initial_points(self):\n",
    "        # Initial datapoints distributed as N(0,1)\n",
    "        self.y[:self.num_prev] = np.random.randn(self.num_prev)\n",
    "\n",
    "    def noise(self):\n",
    "        # Noise distributed as N(0, self.noise_var)\n",
    "        return self.noise_var * np.random.randn()\n",
    "\n",
    "# A set of coefficients that are stable (to produce replicable plots, experiments)\n",
    "fixed_ar_coefficients = {2: [ 0.46152873, -0.29890739],\n",
    "    5: [ 0.02519834, -0.24396899,  0.2785921,   0.14682383,  0.39390468],\n",
    "                        10: [-0.10958935, -0.34564819,  0.3682048,   0.3134046,  -0.21553732,  0.34613629,\n",
    "  0.41916508,  0.0165352,   0.14163503, -0.38844378],\n",
    "                         20: [ 0.1937815,   0.01201026,  0.00464018, -0.21887467, -0.20113385, -0.02322278,\n",
    "  0.34285319, -0.21069086,  0.06604683, -0.22377364,  0.11714593, -0.07122126,\n",
    " -0.16346554,  0.03174824,  0.308584,    0.06881604,  0.24840789, -0.32735569,\n",
    "  0.21939492, 0.3996207 ]}\n",
    "\n",
    "\"\"\"\n",
    "Example of using fixed coefficients (consistency across tests of different models)\n",
    "data = ARData(100, coeffs=fixed_ar_coefficients[5], num_prev=5)\n",
    "plt.plot(data.y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from generate_data import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#####################\n",
    "# Set parameters\n",
    "#####################\n",
    "\n",
    "# Data params\n",
    "noise_var = 0\n",
    "num_datapoints = 100\n",
    "test_size = 0.2\n",
    "num_train = int((1-test_size) * num_datapoints)\n",
    "\n",
    "# Network params\n",
    "input_size = 20\n",
    "# If `per_element` is True, then LSTM reads in one timestep at a time.\n",
    "per_element = True\n",
    "if per_element:\n",
    "    lstm_input_size = 1\n",
    "else:\n",
    "    lstm_input_size = input_size\n",
    "# size of hidden layers\n",
    "h1 = 32\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 500\n",
    "dtype = torch.float\n",
    "\n",
    "#####################\n",
    "# Generate data\n",
    "#####################\n",
    "data = ARData(num_datapoints, num_prev=input_size, test_size=test_size, noise_var=noise_var, coeffs=fixed_ar_coefficients[input_size])\n",
    "\n",
    "# make training and test sets in torch\n",
    "X_train = torch.from_numpy(data.X_train).type(torch.Tensor)\n",
    "X_test = torch.from_numpy(data.X_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(data.y_train).type(torch.Tensor).view(-1)\n",
    "y_test = torch.from_numpy(data.y_test).type(torch.Tensor).view(-1)\n",
    "\n",
    "X_train = X_train.view([input_size, -1, 1])\n",
    "X_test = X_test.view([input_size, -1, 1])\n",
    "\n",
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "\n",
    "# Here we define our model as a class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,\n",
    "                    num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1)\n",
    "\n",
    "model = LSTM(lstm_input_size, h1, batch_size=num_train, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#####################\n",
    "# Train model\n",
    "#####################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if t % 100 == 0:\n",
    "        print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()\n",
    "\n",
    "#####################\n",
    "# Plot preds and performance\n",
    "#####################\n",
    "\n",
    "plt.plot(y_pred.detach().numpy(), label=\"Preds\")\n",
    "plt.plot(y_train.detach().numpy(), label=\"Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10.0\n",
      "1     20.0\n",
      "2     30.0\n",
      "3     40.0\n",
      "4     50.0\n",
      "5     60.0\n",
      "6     70.0\n",
      "7     80.0\n",
      "8     90.0\n",
      "9    100.0\n",
      "dtype: float64\n",
      " Min: 10.000000, Max: 100.000000 \n",
      "[[0.        ]\n",
      " [0.11111111]\n",
      " [0.22222222]\n",
      " [0.33333333]\n",
      " [0.44444444]\n",
      " [0.55555556]\n",
      " [0.66666667]\n",
      " [0.77777778]\n",
      " [0.88888889]\n",
      " [1.        ]]\n",
      "[[ 10.]\n",
      " [ 20.]\n",
      " [ 30.]\n",
      " [ 40.]\n",
      " [ 50.]\n",
      " [ 60.]\n",
      " [ 70.]\n",
      " [ 80.]\n",
      " [ 90.]\n",
      " [100.]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define contrived series\n",
    "data = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n",
    "series = Series(data)\n",
    "print(series)\n",
    "# prepare data for normalization\n",
    "values = series.values\n",
    "values = values.reshape((len(values), 1))\n",
    "# train the normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(values)\n",
    "print(' Min: %f, Max: %f '% (scaler.data_min_, scaler.data_max_))\n",
    "# normalize the dataset and print\n",
    "normalized = scaler.transform(values)\n",
    "print(normalized)\n",
    "# inverse transform and print\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "print(inversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "hidden_dim = 10\n",
    "n_layers = 1\n",
    "\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 1, 10])\n",
      "Hidden:  (tensor([[[ 0.4175, -0.2506, -0.0936, -0.1838,  0.4423,  0.0132,  0.1642,\n",
      "           0.0551,  0.4234, -0.2578]]], grad_fn=<StackBackward>), tensor([[[ 0.9746, -0.5239, -0.2919, -0.3435,  1.9316,  0.0222,  0.2299,\n",
      "           0.1386,  0.8847, -0.8489]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(\"Output shape: \", out.shape)\n",
    "print(\"Hidden: \", hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 24\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0196, -0.2494, -0.0961, -0.1429,  0.2642,  0.0774, -0.0090,  0.1438,\n",
      "        -0.1522,  0.1002], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the last output\n",
    "out = out.squeeze()[-1, :]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\geo\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449911 sha256=6eb058834e80143f231759b2222c333837ffd08c54e13dba16133b521ca1f04b\n",
      "  Stored in directory: C:\\Users\\liuli\\AppData\\Local\\pip\\Cache\\wheels\\96\\86\\f6\\68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\liuli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "train_file = bz2.BZ2File('train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('test.ft.txt.bz2')\n",
    "\n",
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
    "num_test = 200000  # Using 200,000 reviews from test set\n",
    "\n",
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels from sentences\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n",
    "\n",
    "# Some simple cleaning of data\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "2.5% done\n",
      "5.0% done\n",
      "7.5% done\n",
      "10.0% done\n",
      "12.5% done\n",
      "15.0% done\n",
      "17.5% done\n",
      "20.0% done\n",
      "22.5% done\n",
      "25.0% done\n",
      "27.5% done\n",
      "30.0% done\n",
      "32.5% done\n",
      "35.0% done\n",
      "37.5% done\n",
      "40.0% done\n",
      "42.5% done\n",
      "45.0% done\n",
      "47.5% done\n",
      "50.0% done\n",
      "52.5% done\n",
      "55.0% done\n",
      "57.5% done\n",
      "60.0% done\n",
      "62.5% done\n",
      "65.0% done\n",
      "67.5% done\n",
      "70.0% done\n",
      "72.5% done\n",
      "75.0% done\n",
      "77.5% done\n",
      "80.0% done\n",
      "82.5% done\n",
      "85.0% done\n",
      "87.5% done\n",
      "90.0% done\n",
      "92.5% done\n",
      "95.0% done\n",
      "97.5% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    # The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "        words.update([word.lower()])  # Converting all the words to lowercase\n",
    "        train_sentences[i].append(word)\n",
    "    if i%20000 == 0:\n",
    "        print(str((i*100)/num_train) + \"% done\")\n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the words that only appear once\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "# Sorting the words according to the number of appearances, with the most common word being first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words = ['_PAD','_UNK'] + words\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 1 for word in sentence]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # For test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 1 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31,\n",
       " 10,\n",
       " 3,\n",
       " 96,\n",
       " 211,\n",
       " 126,\n",
       " 7118,\n",
       " 103,\n",
       " 16,\n",
       " 7,\n",
       " 211,\n",
       " 5,\n",
       " 76,\n",
       " 26,\n",
       " 78,\n",
       " 262,\n",
       " 13,\n",
       " 1013,\n",
       " 3,\n",
       " 342,\n",
       " 17,\n",
       " 5,\n",
       " 27,\n",
       " 75,\n",
       " 539,\n",
       " 7,\n",
       " 297,\n",
       " 2793,\n",
       " 10,\n",
       " 3,\n",
       " 211,\n",
       " 4,\n",
       " 3,\n",
       " 126,\n",
       " 5,\n",
       " 301,\n",
       " 46,\n",
       " 667,\n",
       " 3,\n",
       " 1570,\n",
       " 8,\n",
       " 18200,\n",
       " 5497,\n",
       " 110,\n",
       " 18,\n",
       " 40,\n",
       " 29,\n",
       " 90,\n",
       " 44,\n",
       " 1576,\n",
       " 59,\n",
       " 8,\n",
       " 333,\n",
       " 3,\n",
       " 948,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 2106,\n",
       " 31,\n",
       " 10,\n",
       " 28,\n",
       " 277,\n",
       " 468,\n",
       " 2,\n",
       " 66,\n",
       " 12,\n",
       " 53,\n",
       " 873,\n",
       " 899,\n",
       " 10,\n",
       " 238,\n",
       " 4,\n",
       " 2005,\n",
       " 4,\n",
       " 6,\n",
       " 1486,\n",
       " 157,\n",
       " 2,\n",
       " 185,\n",
       " 627,\n",
       " 6,\n",
       " 361,\n",
       " 488,\n",
       " 5,\n",
       " 353,\n",
       " 38,\n",
       " 4,\n",
       " 29,\n",
       " 66,\n",
       " 21,\n",
       " 20,\n",
       " 101,\n",
       " 120,\n",
       " 10,\n",
       " 185,\n",
       " 2664,\n",
       " 10,\n",
       " 157,\n",
       " 14,\n",
       " 28,\n",
       " 91,\n",
       " 317,\n",
       " 211,\n",
       " 7118,\n",
       " 2,\n",
       " 5,\n",
       " 187,\n",
       " 1053,\n",
       " 17,\n",
       " 31,\n",
       " 10,\n",
       " 3,\n",
       " 157,\n",
       " 46,\n",
       " 1,\n",
       " 5710,\n",
       " 2257,\n",
       " 44,\n",
       " 58,\n",
       " 1030,\n",
       " 1916,\n",
       " 8,\n",
       " 28,\n",
       " 811,\n",
       " 25,\n",
       " 120,\n",
       " 1,\n",
       " 31,\n",
       " 1422,\n",
       " 55,\n",
       " 11,\n",
       " 948,\n",
       " 12,\n",
       " 17,\n",
       " 41,\n",
       " 127,\n",
       " 668,\n",
       " 59387,\n",
       " 715,\n",
       " 14,\n",
       " 120,\n",
       " 10,\n",
       " 3,\n",
       " 157,\n",
       " 4,\n",
       " 110,\n",
       " 5,\n",
       " 154,\n",
       " 3733,\n",
       " 2,\n",
       " 22,\n",
       " 88,\n",
       " 39,\n",
       " 185,\n",
       " 94,\n",
       " 26,\n",
       " 813,\n",
       " 5,\n",
       " 51,\n",
       " 140,\n",
       " 1012,\n",
       " 3,\n",
       " 372,\n",
       " 177,\n",
       " 9,\n",
       " 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)\n",
    "\n",
    "# Converting our labels into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        print(embeds.shape)\n",
    "        print(hidden[0].shape)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 200, 400])\n",
      "torch.Size([2, 400, 512])\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        break\n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
